import os

from dotenv import load_dotenv
from langchain_core.runnables import ConfigurableField
from langchain_core.runnables.base import RunnableSerializable
from langchain_openai import ChatOpenAI

from killtherobot_backend.models import Answer, Game
from killtherobot_backend.prompts import bots

_ = load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


class GPT_4o(ChatOpenAI):
    """GPT-4o model class derived from ChatOpenAI.

    This class is specifically configured to use the GPT-4o model variant from OpenAI.
    It initializes the model with the necessary API key and model identifier.

    Attributes:
        *args: Variable length argument list.
        **kwargs: Arbitrary keyword arguments.
    """

    def __init__(self, *args, **kwargs):
        kwargs["model"] = "gpt-4o-2024-08-06"
        kwargs["api_key"] = OPENAI_API_KEY
        super().__init__(*args, **kwargs)


def load_model() -> RunnableSerializable:
    """Load and configure a language model for use in chat applications.

    This function initializes a GPT-4o model with configurable alternatives and fields.
    It sets up the model with specific parameters such as temperature and max tokens,
    which can be adjusted as needed. The function returns a configured model instance.

    Returns
    -------
    An instance of a LangChain BaseChatModel.

    Example
    -------
    >>> from chat_app import models
    >>> llm = models.load_model().with_config(
    ...     configurable={
    ...         "llm_temperature": 1.5,
    ...         "llm_max_tokens": 2,
    ...         "llm_model": "gpt-4o",
    ...     }
    ... )
    >>> response = llm.invoke("Where is France?")
    >>> response.content
    'France is'
    """

    llm = (
        GPT_4o()
        .configurable_alternatives(
            ConfigurableField(id="llm_model"),
            default_key="gpt-4o",
            openai=ChatOpenAI(),
        )
        .configurable_fields(
            temperature=ConfigurableField(
                id="llm_temperature",
                name="LLM Temperature",
                description="The temperature of the LLM",
            ),
            max_tokens=ConfigurableField(
                id="llm_max_tokens",
                name="LLM Max Tokens",
                description="The max number of tokens generated by the LLM",
            ),
        )
    )

    return llm


def answer_question(game: Game) -> list[Answer]:
    live_bots = [bot.name for bot in game.bots if bot.isAlive]

    question = game.rounds[-1].question
    prompt_templates = bots
    llm = load_model()

    good_qa_pairs = get_answers(game)

    answers = []
    for bot in live_bots:
        system_message = prompt_templates[bot]["system_message"]
        human_message = prompt_templates[bot]["prompt"].format(
            question=question, good_qa_pairs=good_qa_pairs
        )

        prompt = system_message + "\n\n" + human_message

        answer_text = llm.invoke(prompt).content
        answers.append(Answer(name=bot, text=answer_text, votes=None))

    return answers


def get_answers(game: Game) -> str:
    try:
        live_humans = [human.name for human in game.humans if human.isAlive]

        good_qa_pairs = []
        for i, r in enumerate(game.rounds):
            good_qa_pairs.append(f"Question {i}: {r.question}")
            for j, a in enumerate(r.answers):
                answers = []
                if a.votes is None:
                    continue
                elif any(human in a.votes for human in live_humans):
                    answers.append(f"- Answer {j}: {a.text}")
                good_qa_pairs.append("\n".join(answers))

        good_qa_pairs_str = "\n\n".join(good_qa_pairs)
    except Exception:
        good_qa_pairs_str = ""

    return good_qa_pairs_str


# if __name__ == "__main__":
#     question = "France is in ___"
#     prompt_templates = bots
#     llm = load_model()

#     good_qa_pairs = ""

#     answers = []

#     bot = "John"
#     system_message = prompt_templates[bot]
#     human_message = PromptTemplate.from_template("Say {foo}")
#     human_message.format(good_qa_pairs=good_qa_pairs)

#     prompt = ChatPromptTemplate(
#         [
#             ("system", system_message),
#             ("human", human_message),
#         ]
#     )

#     chain = prompt | llm | StrOutputParser

#     answer_text = chain.invoke({"question": question})
#     answers.append(Answer(name=bot, text=answer_text, votes=None))
#     answers.append(Answer(name=bot, text=answer_text, votes=None))
